@inproceedings{gal2016dropout,
  title={Dropout as a bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016},
  organization={PMLR}
}



%%%%%%%%%%%%%%%% Diffusion models
@misc{ling2022diffusionsurvey,
  url = {https://arxiv.org/abs/2209.00796},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
  year = {2022},
}

@inproceedings{ho2020ddpm,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Denoising Diffusion Probabilistic Models},
 url = {https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf},
 year = {2020}
}

@InProceedings{sohldickstein2015deepunsupervised,
  title = 	 {Deep Unsupervised Learning using Nonequilibrium Thermodynamics},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  year = 	 {2015},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}
@inproceedings{karras2022edm,
  author    = {Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
  title     = {Elucidating the Design Space of Diffusion-Based Generative Models},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022}
}

@article{song2017pixeldefend,
  title   = {Pixeldefend: Leveraging generative models to understand and defend against adversarial examples},
  author  = {Song, Yang and Kim, Taesup and Nowozin, Sebastian and Ermon, Stefano and Kushman, Nate},
  journal = {arXiv preprint arXiv:1710.10766},
  year    = {2017}
}
@inproceedings{song2019generative,
  title     = {Generative modeling by estimating gradients of the data distribution},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {32},
  year      = {2019}
}
@inproceedings{song2019sliced,
  author    = {Yang Song and
               Sahaj Garg and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial
               Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pages     = {204},
  year      = {2019},
  url       = {http://auai.org/uai2019/proceedings/papers/204.pdf}
}
@inproceedings{song2021ddim,
  title     = {Denoising Diffusion Implicit Models},
  author    = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}
@inproceedings{song2020improved,
  title     = {Improved techniques for training score-based generative models},
  author    = {Song, Yang and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {33},
  pages     = {12438--12448},
  year      = {2020}
}
@inproceedings{song2020scoreSDE,
  title     = {Score-Based Generative Modeling through Stochastic Differential Equations},
  author    = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}
@inproceedings{song2021maximum,
  title     = {Maximum likelihood training of score-based diffusion models},
  author    = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  volume    = {34},
  pages     = {1415--1428},
  year      = {2021}
}
@inproceedings{song2021solving,
  title     = {Solving Inverse Problems in Medical Imaging with Score-Based Generative Models},
  author    = {Song, Yang and Shen, Liyue and Xing, Lei and Ermon, Stefano},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}
@inproceedings{lou2023reflected,
      title={Reflected Diffusion Models},
      author={Aaron Lou and Stefano Ermon},
      booktitle={International Conference on Machine Learning},
      year={2023},
}
@article{song2021train,
  title   = {How to train your energy-based models},
  author  = {Song, Yang and Kingma, Diederik P},
  journal = {arXiv preprint arXiv:2101.03288},
  year    = {2021}
}
@article{song2022applying,
  title   = {Applying Regularized {S}chr{ö}dinger-Bridge-Based Stochastic Process in Generative Modeling},
  author  = {Song, Ki-Ung},
  journal = {arXiv preprint arXiv:2208.07131},
  year    = {2022}
}

@inproceedings{bansal2022cold,
  title={Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise},
  author={Arpit Bansal and Eitan Borgnia and Hong-Min Chu and Jie S. Li and Hamid Kazemi and Furong Huang and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  year={2023},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  url={https://arxiv.org/abs/2208.09392}
}

%%%%%%%%%%% Video diffusion models
@inproceedings{voleti2022mcvd,
  title={MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation},
  url = {https://arxiv.org/abs/2205.09853},
  author = {Voleti, Vikram and Jolicoeur-Martineau, Alexia and Pal, Christopher},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year = {2022}
}

@article{ho2022videodiffusion,
    title={Video diffusion models},
    author={Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J},
    journal={Advances in Neural Information Processing Systems (NeurIPS)},
    year={2022}}
}

@article{singer2022makeavideo,
  url = {https://arxiv.org/abs/2209.14792},
  author = {Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and Parikh, Devi and Gupta, Sonal and Taigman, Yaniv},
  title = {Make-A-Video: Text-to-Video Generation without Text-Video Data},
  year = {2022}
}

@article{ho2022imagenvideo,
  url = {https://arxiv.org/abs/2210.02303},
  author = {Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P. and Poole, Ben and Norouzi, Mohammad and Fleet, David J. and Salimans, Tim},
  title = {Imagen Video: High Definition Video Generation with Diffusion Models},
  year = {2022}
}
@article{harvey2022flexiblevideos,
  title   = {Flexible Diffusion Modeling of Long Videos},
  author  = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  journal = {Advances in Neural Information Processing Systems (NeurIPS)},
  year    = {2022}
}

@article{yang2022diffusion,
  title   = {Diffusion probabilistic modeling for video generation},
  author  = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
  journal = {arXiv preprint arXiv:2203.09481},
  year    = {2022}
}


%%%%%%%%%%%% DATA
@article{Rasp2020weatherbench,
	url = {https://doi.org/10.1029\%2F2020ms002203},
	year = {2020},
	author = {Stephan Rasp and Peter D. Dueben and Sebastian Scher and Jonathan A. Weyn and Soukayna Mouatadid and Nils Thuerey},
	title = {{WeatherBench}: A Benchmark Data Set for Data-Driven Weather Forecasting},
	journal = {Journal of Advances in Modeling Earth Systems}
}
@article{otness21nnbenchmark,
  title={An Extensible Benchmark Suite for Learning to Simulate Physical Systems},
  author={Karl Otness and Arvi Gjoka and Joan Bruna and Daniele Panozzo and Benjamin Peherstorfer and Teseo Schneider and Denis Zorin},
  year={2021},
  url={https://arxiv.org/abs/2108.07799},
  journal={Advances in Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks}
}

@article{huang2021oisstv2,
      author = "Boyin Huang and Chunying Liu and Viva Banzon and Eric Freeman and Garrett Graham and Bill Hankins and Tom Smith and Huai-Min Zhang",
      title = "Improvements of the Daily Optimum Interpolation Sea Surface Temperature (DOISST) Version 2.1",
      journal = "Journal of Climate",
      year = "2021",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "34",
      number = "8",
      pages=      "2923 - 2939",
      url = "https://journals.ametsoc.org/view/journals/clim/34/8/JCLI-D-20-0166.1.xml"
}

%  Deep learning weather forecasting
@article{pathak2022fourcastnet,
  url = {https://arxiv.org/abs/2202.11214},
  author = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
  title = {FourCastNet: A Global Data-driven High-resolution Weather Model using Adaptive Fourier Neural Operators},
  year = {2022},
}   % At short term similarly good or better than IFS. Training is done on next-step predictions, and then fine-tuned on unrolled step 1 and 2 predictions

@article{lam2022graphcast,
  url = {https://arxiv.org/abs/2212.12794},
  author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
  title = {{GraphCast}: Learning skillful medium-range global weather forecasting},
  year = {2022},
}

@article{bi2022pangu,
  title={Pangu-Weather: A 3D High-Resolution Model for Fast and Accurate Global Weather Forecast},
  author={Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  journal={arXiv preprint arXiv:2211.02556},
  year={2022}
} % Hierarchical lead-time models so that mid-range forecasts require few NN calls. Ensembling via input perturbations like fourcastnet

@article{nguyen2023climax,
  title={Clima{X}: A foundation model for weather and climate},
  author={Nguyen, Tung and Brandstetter, Johannes and Kapoor, Ashish and Gupta, Jayesh K and Grover, Aditya},
  journal={International Conference on Machine Learning},
  year={2023}
}


@inproceedings{de2018physicalsstbaseline,
    title={Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge},
    author={Emmanuel de Bezenac and Arthur Pajot and Patrick Gallinari},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=By4HsfWAZ},
}

@inproceedings{bezenac2020normalizing,
 author = {de Bezenac, Emmanuel and Rangapuram, Syama Sundar and Benidis, Konstantinos and Bohlke-Schneider, Michael and Kurle, Richard and Stella, Lorenzo and Hasson, Hilaf and Gallinari, Patrick and Januschowski, Tim},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {2995--3007},
 title = {Normalizing Kalman Filters for Multivariate Time Series Analysis},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1f47cef5e38c952f94c5d61726027439-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{sanchezgonzalez2020learning,
  title={Learning to Simulate Complex Physics with Graph Networks},
  author={Alvaro Sanchez-Gonzalez and
          Jonathan Godwin and
          Tobias Pfaff and
          Rex Ying and
          Jure Leskovec and
          Peter W. Battaglia},
  booktitle={International Conference on Machine Learning},
  year={2020}
}  % Add noise to the training data to attain stable rollouts (so that input distribution more closely to error-prone rollout distributions). The model predicts and is trained for next-step prediction only.


@article{weyn2019canmachines,
    author = {Weyn, Jonathan A. and Durran, Dale R. and Caruana, Rich},
    title = {Can Machines Learn to Predict Weather? Using Deep Learning to Predict Gridded 500-hPa Geopotential Height From Historical Weather Data},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {11},
    number = {8},
    pages = {2680-2693},
    keywords = {machine learning, weather prediction, deep learning, neural network},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001705},
    year = {2019}
}

@article{rasp2018postprocessing,
      author = "Stephan Rasp and Sebastian Lerch",
      title = "Neural Networks for Postprocessing Ensemble Weather Forecasts",
      journal = "Monthly Weather Review",
      year = "2018",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "146",
      number = "11",
      pages=   "3885 - 3900",
}

@article{rasp2021datadriven,
    author = {Rasp, Stephan and Thuerey, Nils},
    title = {Data-Driven Medium-Range Weather Prediction With a Resnet Pretrained on Climate Simulations: A New Model for WeatherBench},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {13},
    number = {2},
    keywords = {deep learning, machine learning, numerical weather forecasting},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002405},
    year = {2021}
}


@inproceedings{chattopadhyay2020deep,
  title={Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence},
  author={Chattopadhyay, Ashesh and Mustafa, Mustafa and Hassanzadeh, Pedram and Kashinath, Karthik},
  booktitle={Proceedings of the 10th International Conference on Climate Informatics},
  pages={106--112},
  year={2020}
}

@article{chattopadhyay2022why,
  title={Why are deep learning-based models of geophysical turbulence long-term unstable? },
  author={Chattopadhyay, Ashesh and Hassanzadeh, Pedram},
  journal={NeurIPS, Machine Learning for Physical Sciences},
  year={2022}
}

@article{keisler2022forecasting,
  url = {https://arxiv.org/abs/2202.07575},
  author = {Keisler, Ryan},
  title = {Forecasting Global Weather with Graph Neural Networks},
  year = {2022},
}


@article{erichson2019pimlLyapunov,
  url = {https://arxiv.org/abs/1905.10866},
  author = {Erichson, N. Benjamin and Muehlebach, Michael and Mahoney, Michael W.},
  title = {Physics-informed Autoencoders for Lyapunov-stable Fluid Flow Prediction},
  year = {2019},
}

@article{mamakoukas2020learningstable,
    author = {Mamakoukas, Giorgos and Abraham, Ian and Murphey, Todd},
    year = {2020},
    journal = {IEEE Transactions on Robotics},
    title = {Learning Stable Models for Prediction and Control}
}

%%%%%%% DL For TIMESERIES
@inproceedings{Rasul2021AutoregressiveDD,
  title={Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting},
  author={Kashif Rasul and Calvin Seward and Ingmar Schuster and Roland Vollgraf},
  booktitle={ICML},
  year={2021}
}

@article{lopez2023diffusionimputation,
    title={Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models},
    author={Juan Lopez Alcaraz and Nils Strodthoff},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=hHiIbk7ApW},
}

@inproceedings{wu2021quantifying,
  title={Quantifying uncertainty in deep spatiotemporal forecasting},
  author={Wu, Dongxia and Gao, Liyao and Chinazzi, Matteo and Xiong, Xinyue and Vespignani, Alessandro and Ma, Yi-An and Yu, Rose},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1841--1851},
  year={2021}
}

% Probabilistic weather forecasting
@article{scher2021ensemble,
    author = {Scher, Sebastian and Messori, Gabriele},
    title = {Ensemble Methods for Neural Network-Based Weather Forecasts},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {13},
    number = {2},
    doi = {https://doi.org/10.1029/2020MS002331},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020MS002331},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2020MS002331},
    abstract = {Abstract Ensemble weather forecasts enable a measure of uncertainty to be attached to each forecast, by computing the ensemble's spread. However, generating an ensemble with a good spread-error relationship is far from trivial, and a wide range of approaches to achieve this have been explored—chiefly in the context of numerical weather prediction models. Here, we aim to transform a deterministic neural network weather forecasting system into an ensemble forecasting system. We test four methods to generate the ensemble: random initial perturbations, retraining of the neural network, use of random dropout in the network, and the creation of initial perturbations with singular vector decomposition. The latter method is widely used in numerical weather prediction models, but is yet to be tested on neural networks. The ensemble mean forecasts obtained from these four approaches all beat the unperturbed neural network forecasts, with the retraining method yielding the highest improvement. However, the skill of the neural network forecasts is systematically lower than that of state-of-the-art numerical weather prediction models.},
    year = {2021}
}
% scher2021ensemble studies 4 methods to make a deterministic NN probabilistic (initial conditions, inference dropout, and random seeds). All models are next-step (=6h) forecasters, that are unrolled autoregressively

@article{garg2022weatherbenchprob,
  title={WeatherBench Probability: A benchmark dataset for probabilistic medium-range weather forecasting along with deep learning baseline models},
  author={Garg, Sagar and Rasp, Stephan and Thuerey, Nils},
  journal={arXiv preprint arXiv:2205.00865},
  year={2022}
}  % garg2022weatherbenchprob studies proba forecasts based on inference dropout, Gaussian parametric, and by discretizing into bins

@article{hu2023swinrnn,
    author = {Hu, Yuan and Chen, Lei and Wang, Zhibin and Li, Hao},
    title = {SwinVRNN: A Data-Driven Ensemble Forecasting Model via Learned Distribution Perturbation},
    journal = {Journal of Advances in Modeling Earth Systems},
    volume = {15},
    number = {2},
    pages = {e2022MS003211},
    keywords = {medium-range weather forecasting, data-driven method, ensemble forecast, learned distribution perturbation},
    doi = {https://doi.org/10.1029/2022MS003211},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2022MS003211},
    eprint = {https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2022MS003211},
    note = {e2022MS003211 2022MS003211},
    abstract = {Abstract The data-driven approaches for medium-range weather forecasting are recently shown to be extraordinarily promising for ensemble forecasting due to their fast inference speed compared to the traditional numerical weather prediction models. However, their forecast accuracy can hardly match the state-of-the-art operational ECMWF Integrated Forecasting System (IFS) model. Previous data-driven approaches perform ensemble forecasting using some simple perturbation methods, like the initial condition perturbation and the Monte Carlo dropout. However, their ensemble performance is often limited arguably by the sub-optimal ways of applying perturbation. We propose a Swin Transformer-based Variational Recurrent Neural Network (SwinVRNN), which is a stochastic weather forecasting model combining a SwinRNN predictor with a perturbation module. SwinRNN is designed as a Swin Transformer-based recurrent neural network, which predicts the future states deterministically. Furthermore, to model the stochasticity in the prediction, we design a perturbation module following the Variational Auto-Encoder paradigm to learn the multivariate Gaussian distributions of a time-variant stochastic latent variable from the data. Ensemble forecasting can be easily performed by perturbing the model features leveraging the noise sampled from the learned distribution. We also compare four categories of perturbation methods for ensemble forecasting, that is, fixed distribution perturbation, learned distribution perturbation, MC dropout, and multi model ensemble. Comparisons on the WeatherBench data set show that the learned distribution perturbation method using our SwinVRNN model achieves remarkably improved forecasting accuracy and reasonable ensemble spread due to the joint optimization of the two targets. More notably, SwinVRNN surpasses operational IFS on the surface variables of the 2-m temperature and the 6-hourly total precipitation at all lead times up to 5 days (Code is available at https://github.com/tpys/wwprediction).},
    year = {2023}
}
% SwinRNN adds stochasticity to the deterministic RNN (next-step predictor) by perturbing a latent condition with a VAE


@article{thuemmel2023inductive,
      title={Inductive biases in deep learning models for weather prediction},
      author={Jannik Thuemmel and Matthias Karlbauer and Sebastian Otte and Christiane Zarfl and Georg Martius and Nicole Ludwig and Thomas Scholten and Ulrich Friedrich and Volker Wulfmeyer and Bedartha Goswami and Martin V. Butz},
      year={2023},
      eprint={2304.04664},
      archivePrefix={arXiv},
}


@article{300BillionServed2009,
      author = "Jeffrey K. Lazo and Rebecca E. Morss and Julie L. Demuth",
      title = "300 Billion Served: Sources, Perceptions, Uses, and Values of Weather Forecasts",
      journal = "Bulletin of the American Meteorological Society",
      year = "2009",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      volume = "90",
      number = "6",
      pages=      "785 - 798",
      url = "https://journals.ametsoc.org/view/journals/bams/90/6/2008bams2604_1.xml"
}

@article{gneiting2005weather,
    author = {Tilmann Gneiting  and Adrian E. Raftery },
    title = {Weather Forecasting with Ensemble Methods},
    journal = {Science},
    volume = {310},
    number = {5746},
    pages = {248-249},
    year = {2005},
    URL = {https://www.science.org/doi/abs/10.1126/science.1115255},
}

@article{gneiting2014Probabilistic,
    author = {Gneiting, Tilmann and Katzfuss, Matthias},
    title = {Probabilistic Forecasting},
    journal = {Annual Review of Statistics and Its Application},
    volume = {1},
    number = {1},
    pages = {125-151},
    year = {2014}
}

%%%%%%%%%%%%%%%%

@article{bevacqua2023smiles,
  title    = {Advancing research on compound weather and climate events via large ensemble model simulations},
  author   = {Bevacqua, Emanuele and Suarez-Gutierrez, Laura and Jezequel, Aglae and Lehner, Flavio and Vrac, Mathieu
              and Yiou, Pascal and Zscheischler, Jakob},
  journal  = {Nature Communications},
  volume   =  {14},
  number   =  {1},
  pages    = {2145},
  year     = {2023}
}

@article{el2021implicit,
  title={Implicit deep learning},
  author={El Ghaoui, Laurent and Gu, Fangda and Travacca, Bertrand and Askari, Armin and Tsai, Alicia},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={3},
  number={3},
  pages={930--958},
  year={2021},
  publisher={SIAM}
}


@article{kochkov2021mlfluids,
    author = {Dmitrii Kochkov  and Jamie A. Smith  and Ayya Alieva  and Qing Wang  and Michael P. Brenner  and Stephan Hoyer},
    title = {Machine learning–accelerated computational fluid dynamics},
    journal = {Proceedings of the National Academy of Sciences},
    volume = {118},
    number = {21},
    pages = {e2101784118},
    year = {2021},
    doi = {10.1073/pnas.2101784118},
    URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2101784118}
}

@inproceedings{graubner2022calibration,
  title={Calibration of Large Neural Weather Models},
  author={Graubner, Andre and Kamyar Azizzadenesheli, Kamyar and Pathak, Jaideep and Mardani, Morteza and Pritchard, Mike and Kashinath, Karthik and Anandkumar, Anima},
  booktitle={NeurIPS 2022 Workshop on Tackling Climate Change with Machine Learning},
  year={2022}
}
@inproceedings{han2022predicting,
    title={Predicting Physics in Mesh-reduced Space with Temporal Attention},
    author={Xu Han and Han Gao and Tobias Pfaff and Jian-Xun Wang and Liping Liu},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=XctLdNfCmP}
}  % Maps snapshots to latent (graph-based) space (basically to latent tokens), on which an autoregressive transformer operates to evolve the dynamics up to T snapshots into the future (using the previously predicted tokens as input/query). Interesting is that they propose a multi-step loss curriculum, i.e the model "is trained by incrementally including loss terms computed from different time steps." (i.e. it seems, first only loss for x1, then x1 and x2, then x1 x2 x3, etc.). Issue: Needs to attend during training on whole simulation trajectories.

@inproceedings{brandstetter2022message,
    title={Message Passing Neural PDE Solvers},
    author={Johannes Brandstetter and Daniel E. Worrall and Max Welling},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=vSix3HPYKSU}
} % Basically, does autoregressive rollouts but over direct multi-step predictions ("temporal bundling") and adds a stability loss term on rolled out predictions (two steps in exps) based on a domain adaptation adversarial loss

@inproceedings{brandstetter2023clifford,
    title={Clifford Neural Layers for PDE Modeling},
    author={Johannes Brandstetter and Rianne van den Berg and Max Welling and Jayesh K Gupta},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=okwxL_c4x84}
}

@inproceedings{janny2023eagle,
    title={EAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers},
    author={Steeven Janny and Aur{\'e}lien B{\'e}n{\'e}teau and Madiha Nadri and Julie Digne and Nicolas Thome and Christian Wolf},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=mfIX4QpsARJ}
}

@article{wang2020tfnet,
   title={Towards Physics-informed Deep Learning for Turbulent Flow Prediction},
   author={Rui Wang and Karthik Kashinath and Mustafa Mustafa and Adrian Albert and Rose Yu},
   journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
   year = {2020}
}
% To generate multi-step forecasts, \cite{wang2020tfnet} perform one-step ahead prediction and roll out the predictions autoregressively

@inproceedings{wang2022metalearning,
      title={Meta-Learning Dynamics Forecasting Using Task Inference},
      author={Rui Wang and Robin Walters and Rose Yu},
      year={2022},
      booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{wan2023evolve,
    title={Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics For Advection-Dominated Systems},
    author={Zhong Yi Wan and Leonardo Zepeda-Nunez and Anudhyan Boral and Fei Sha},
    booktitle={The Eleventh International Conference on Learning Representations},
    year={2023},
    url={https://openreview.net/forum?id=Z4s73sJYQM}
}


@inproceedings{tran2023factorized,
    title={Factorized Fourier Neural Operators},
    author={Alasdair Tran and Alexander Mathews and Lexing Xie and Cheng Soon Ong},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=tmIiMPl4IPa}
}

@inproceedings{zhang2018mixup,
    title={mixup: Beyond Empirical Risk Minimization},
    author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2018},
    url={https://openreview.net/forum?id=r1Ddp1-Rb},
}

@article{tu2022ccaiautoml,
  url = {https://arxiv.org/abs/2210.03324},
  author = {Tu, Renbo and Roberts, Nicholas and Prasad, Vishak and Nayak, Sibasis and Jain, Paarth and Sala, Frederic and Ramakrishnan, Ganesh and Talwalkar, Ameet and Neiswanger, Willie and White, Colin},
  title = {AutoML for Climate Change: A Call to Action},
  year = {2022},
}

@Article{liu2022convnext,
  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
  title   = {A ConvNet for the 2020s},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2022},
}

%%%%%%%%%%%% GENERATIVE MODELS FOR CLIMATE DATA

@article{harris2022generative,
  title={A Generative Deep Learning Approach to Stochastic Downscaling of Precipitation Forecasts},
  author={Harris, Lucy and McRae, Andrew TT and Chantry, Matthew and Dueben, Peter D and Palmer, Tim N},
  journal={arXiv preprint arXiv:2204.02028},
  year={2022}
}

@article{ravuri2021skilful,
  title={Skilful precipitation nowcasting using deep generative models of radar},
  author={Ravuri, Suman and Lenc, Karel and Willson, Matthew and Kangin, Dmitry and Lam, Remi and Mirowski, Piotr and Fitzsimons, Megan and Athanassiadou, Maria and Kashem, Sheleem and Madge, Sam and others},
  journal={Nature},
  volume={597},
  number={7878},
  pages={672--677},
  year={2021},
  publisher={Nature Publishing Group},
  url={https://doi.org/10.1038/s41586-021-03854-z}
}
% ravuri2021skilful is based on multi-step predicting GANs
% Note on their Unet baseline: "rather than  predicting only a single output and using autoregressive sampling
% during evaluation, the model predicts all frames in a single forward pass. This somewhat mitigates the excessive blurring found in ref. 5 and improves results on quantitative evaluation"


@article{sonderby2020metnet,
  title={Metnet: A neural weather model for precipitation forecasting},
  author={S{\o}nderby, Casper Kaae and Espeholt, Lasse and Heek, Jonathan and Dehghani, Mostafa and Oliver, Avital and Salimans, Tim and Agrawal, Shreya and Hickey, Jason and Kalchbrenner, Nal},
  journal={arXiv preprint arXiv:2003.12140},
  year={2020}
}

@article{espeholt2022metnet2,
    author={Espeholt, Lasse and Agrawal, Shreya and S{\o}nderby, Casper and Kumar, Manoj and Heek, Jonathan and Bromberg, Carla
    and Gazen, Cenk and Carver, Rob and Andrychowicz, Marcin and Hickey, Jason and Bell, Aaron and Kalchbrenner, Nal},
    title={Deep learning for twelve hour precipitation forecasts},
    journal={Nature Communications},
    year={2022},
    month={Sep},
    day={01},
    volume={13},
    number={1},
    pages={5145},
    issn={2041-1723},
    doi={10.1038/s41467-022-32483-x},
    url={https://doi.org/10.1038/s41467-022-32483-x}
}



%%%%%%%%%%%%%%%%% OTHER

@article{bauer2015thequiet,
    author={Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
    title={The quiet revolution of numerical weather prediction},
    journal={Nature},
    year={2015},
    month={Sep},
    day={01},
    volume={525},
    number={7567},
    pages={47-55},
    issn={1476-4687},
    doi={10.1038/nature14956},
    url={https://doi.org/10.1038/nature14956}
}


@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hersbach2000decomposition,
  title={Decomposition of the continuous ranked probability score for ensemble prediction systems},
  author={Hersbach, Hans},
  journal={Weather and Forecasting},
  volume={15},
  number={5},
  pages={559--570},
  year={2000},
  publisher={American Meteorological Society}
}

@inproceedings{unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical image computing and computer-assisted intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@inproceedings{cnn,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}



% Multi step forecasting
@inproceedings{lambert21longtermRL,
  author={Lambert, Nathan and Wilcox, Albert and Zhang, Howard and Pister, Kristofer S. J. and Calandra, Roberto},
  title={Learning Accurate Long-term Dynamics for Model-based Reinforcement Learning},
  booktitle={IEEE Conference on Decision and Control (CDC)},
  year={2021}
}


@article{ham2019dlenso,
    author = {Ham, Yoo-Geun and Kim, Jeong-Hwan and Luo, Jing-Jia},
    year = {2019},
    month = {9},
    title = {Deep learning for multi-year ENSO forecasts},
    journal = {Nature},
    pages = {568-572},
    volume = {573},
    issue = {7775}
}

@article{vos2021long,
  title={Long-range seasonal forecasting of 2m-temperature with machine learning},
  author={Vos, Etienne E and Gritzman, Ashley and Makhanya, Sibusisiwe and Mashinini, Thabang and Watson, Campbell D},
  journal={NeurIPS, Tackling Climate Change with Machine Learning},
  year={2020}
}

@article{raissi2017pidl,
  url = {https://arxiv.org/abs/1711.10561},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  title = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
  year = {2017},
}


@article{kashinath2021piml,
    author = {Kashinath, K.  and Mustafa, M.  and Albert, A.  and Wu, J-L.  and Jiang, C.  and Esmaeilzadeh, S.  and Azizzadenesheli, K.  and Wang, R.  and Chattopadhyay, A.  and Singh, A.  and Manepalli, A.  and Chirila, D.  and Yu, R.  and Walters, R.  and White, B.  and Xiao, H.  and Tchelepi, H. A.  and Marcus, P.  and Anandkumar, A.  and Hassanzadeh, P.  and Prabhat, null },
    title = {Physics-informed machine learning: case studies for weather and climate modelling},
    journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
    volume = {379},
    number = {2194},
    pages = {20200093},
    year = {2021},
    doi = {10.1098/rsta.2020.0093},
    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2020.0093},
}

@article{brenowitz2018prognostic,
    author = {Brenowitz, N. D. and Bretherton, C. S.},
    title = {Prognostic Validation of a Neural Network Unified Physics Parameterization},
    journal = {Geophysical Research Letters},
    volume = {45},
    number = {12},
    pages = {6289-6298},
    year = {2018}
}

@article{scher2018toward,
    author = {Scher, S.},
    title = {Toward Data-Driven Weather and Climate Forecasting: Approximating a Simple General Circulation Model With Deep Learning},
    journal = {Geophysical Research Letters},
    volume = {45},
    number = {22},
    keywords = {machine learning, weather prediction, neural networks, deep learning, climate models},
    doi = {https://doi.org/10.1029/2018GL080704},
    url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2018GL080704},
    year = {2018}
}

@article{scher2019generalization,
    author = {Scher, S. and Messori, G.},
    title = {Generalization properties of feed-forward neural networks trained on Lorenz systems},
    journal = {Nonlinear Processes in Geophysics},
    volume = {26},
    year = {2019},
    number = {4},
    pages = {381--399},
    url = {https://npg.copernicus.org/articles/26/381/2019/},
    doi = {10.5194/npg-26-381-2019}
}

%%%%%%%%%% Metrics

@article{matheson1976crps,
 author = {James E. Matheson and Robert L. Winkler},
 journal = {Management Science},
 number = {10},
 pages = {1087--1096},
 publisher = {INFORMS},
 title = {Scoring Rules for Continuous Probability Distributions},
 volume = {22},
 year = {1976}
}


@article{matheson1976scoring,
  title={Scoring rules for continuous probability distributions},
  author={Matheson, James E and Winkler, Robert L},
  journal={Management science},
  volume={22},
  number={10},
  pages={1087--1096},
  year={1976},
  publisher={INFORMS}
}


@article{fortin2014ssr,
      author = {V.  Fortin and M.  Abaza and F.  Anctil and R.  Turcotte},
      title = {Why Should Ensemble Spread Match the RMSE of the Ensemble Mean?},
      journal = {Journal of Hydrometeorology},
      year = {2014},
      publisher = {American Meteorological Society"},
      doi = {https://doi.org/10.1175/JHM-D-14-0008.1},
      url = {https://journals.ametsoc.org/view/journals/hydr/15/4/jhm-d-14-0008_1.xml}
}

%%%%%% DYNAMICS FORECASTING APPLICATIONS %%%%%%

@article{leggett2020united,
  title={The united nations framework convention on climate change, the Kyoto protocol, and the Paris agreement: a summary},
  author={Leggett, Jane A},
  journal={UNFCC: New York, NY, USA},
  volume={2},
  year={2020}
}

@article{labadie2004optimal,
  title={Optimal operation of multireservoir systems: State-of-the-art review},
  author={Labadie, John W},
  journal={Journal of water resources planning and management},
  volume={130},
  number={2},
  pages={93--111},
  year={2004},
  publisher={American Society of Civil Engineers}
}

@article{brown2015future,
  title={The future of water resources systems analysis: Toward a scientific framework for sustainable water management},
  author={Brown, Casey M and Lund, Jay R and Cai, Ximing and Reed, Patrick M and Zagona, Edith A and Ostfeld, Avi and Hall, Jim and Characklis, Gregory W and Yu, Winston and Brekke, Levi},
  journal={Water resources research},
  volume={51},
  number={8},
  pages={6110--6124},
  year={2015},
  publisher={Wiley Online Library}
}

@article{preisler2007statistical,
  title={Statistical model for forecasting monthly large wildfire events in western United States},
  author={Preisler, Haiganoush K and Westerling, Anthony L},
  journal={Journal of Applied Meteorology and Climatology},
  volume={46},
  number={7},
  pages={1020--1030},
  year={2007}
}

@article{westerling2003climate,
  title={Climate and wildfire in the western United States},
  author={Westerling, Anthony L and Gershunov, Alexander and Brown, Timothy J and Cayan, Daniel R and Dettinger, Michael D},
  journal={Bulletin of the American Meteorological Society},
  volume={84},
  number={5},
  pages={595--604},
  year={2003},
  publisher={American Meteorological Society}
}